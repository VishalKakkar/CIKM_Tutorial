{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle,sys\n",
    "import datetime\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/narre_cnn.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_block(embedded_reviews,embedding_size,filter_sizes,num_filters,name='user_conv-maxpool'):\n",
    "    pooled_outputs = []\n",
    "    for i, filter_size in enumerate(filter_sizes):\n",
    "        with tf.name_scope(name+\"-%s\" % filter_size):\n",
    "                # Convolution Layer\n",
    "            filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "            W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "            embedded_reviews = tf.reshape(embedded_reviews, [-1, review_len_u, embedding_size, 1])\n",
    "\n",
    "            conv = tf.nn.conv2d(\n",
    "                    embedded_reviews,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "            h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                # Maxpooling over the outputs\n",
    "            pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, review_len_u - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "            pooled_outputs.append(pooled)\n",
    "    return pooled_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/narre_user.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_attetion_block(h_drop_u,iid_a,attention_size,embedding_id,num_filters_total,l2_loss):\n",
    "    Wau = tf.Variable(\n",
    "                tf.random_uniform([num_filters_total, attention_size], -0.1, 0.1), name='Wau')\n",
    "    \n",
    "    Wru = tf.Variable(\n",
    "                tf.random_uniform([embedding_id, attention_size], -0.1, 0.1), name='Wru')\n",
    "    \n",
    "    Wpu = tf.Variable(\n",
    "                tf.random_uniform([attention_size, 1], -0.1, 0.1), name='Wpu')\n",
    "    \n",
    "    bau = tf.Variable(tf.constant(0.1, shape=[attention_size]), name=\"bau\")\n",
    "    bbu = tf.Variable(tf.constant(0.1, shape=[1]), name=\"bbu\")\n",
    "#     self.iid_a = tf.nn.relu(tf.nn.embedding_lookup(iidW, self.input_reuid))\n",
    "    u_j = tf.einsum('ajk,kl->ajl', tf.nn.relu(\n",
    "                tf.einsum('ajk,kl->ajl', h_drop_u, Wau) + tf.einsum('ajk,kl->ajl', iid_a, Wru) + bau),\n",
    "                                             Wpu)+bbu  # None*u_len*1\n",
    "\n",
    "    u_a = tf.nn.softmax(u_j,1)  # none*u_len*1\n",
    "\n",
    "    \n",
    "    l2_loss += tf.nn.l2_loss(Wau)\n",
    "    l2_loss += tf.nn.l2_loss(Wru)\n",
    "    \n",
    "    return u_a,l2_loss\n",
    "\n",
    "def user_path(input_u, input_uid, input_reuid, iidW, dropout_keep_prob,review_num_u,user_num,user_vocab_size,\n",
    "              attention_size,embedding_size,embedding_id,n_latent,filter_sizes,num_filters,l2_loss):\n",
    "    with tf.name_scope(\"user_embedding\"):\n",
    "        W1 = tf.Variable(\n",
    "                    tf.random_uniform([user_vocab_size, embedding_size], -1.0, 1.0),\n",
    "                    name=\"W1\")\n",
    "        embedded_user = tf.nn.embedding_lookup(W1, input_u)\n",
    "        embedded_users = tf.expand_dims(embedded_user, -1)\n",
    "        \n",
    "    pooled_outputs_u = cnn_block(embedded_users,embedding_size,filter_sizes,num_filters,name='user_conv-maxpool')       \n",
    "    num_filters_total = num_filters * len(filter_sizes)\n",
    "    h_pool_u = tf.concat(pooled_outputs_u,axis=-1)\n",
    "    h_pool_flat_u = tf.reshape(h_pool_u, [-1, review_num_u, num_filters_total])\n",
    "        \n",
    "    with tf.name_scope(\"dropout\"):\n",
    "        h_drop_u = tf.nn.dropout(h_pool_flat_u, 1.0)\n",
    "            \n",
    "    with tf.name_scope(\"attention\") as attention_scope:\n",
    "        iid_a = tf.nn.relu(tf.nn.embedding_lookup(iidW, input_reuid))\n",
    "        u_a,l2_loss = user_attetion_block(h_drop_u,iid_a,attention_size,embedding_id,num_filters_total,l2_loss)\n",
    "            \n",
    "    with tf.name_scope(\"add_reviews\"):\n",
    "        u_feas = tf.reduce_sum(tf.multiply(u_a, h_drop_u), 1)\n",
    "        u_feas = tf.nn.dropout(u_feas, dropout_keep_prob)\n",
    "        \n",
    "    with tf.name_scope(\"get_fea\") as get_fea_scope:\n",
    "        uidmf = tf.Variable(tf.random_uniform([user_num + 2, embedding_id], -0.1, 0.1), name=\"uidmf\")\n",
    "        uid = tf.nn.embedding_lookup(uidmf,input_uid)    \n",
    "        uid = tf.reshape(uid,[-1,embedding_id])\n",
    "            \n",
    "        Wu = tf.Variable(\n",
    "                tf.random_uniform([num_filters_total, n_latent], -0.1, 0.1), name='Wu')\n",
    "        bu = tf.Variable(tf.constant(0.1, shape=[n_latent]), name=\"bu\")\n",
    "        \n",
    "        u_feas = tf.matmul(u_feas, Wu)+uid + bu\n",
    "\n",
    "    return u_a,u_feas,l2_loss,attention_scope,get_fea_scope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/narre_item.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_attetion_block(h_drop_i,uid_a,attention_size,embedding_id,num_filters_total,l2_loss):\n",
    "    Wai = tf.Variable(\n",
    "                tf.random_uniform([num_filters_total, attention_size], -0.1, 0.1), name='Wai')\n",
    "    \n",
    "    Wri = tf.Variable(\n",
    "                tf.random_uniform([embedding_id, attention_size], -0.1, 0.1), name='Wri')\n",
    "    \n",
    "    Wpi = tf.Variable(\n",
    "                tf.random_uniform([attention_size, 1], -0.1, 0.1), name='Wpi')\n",
    "    \n",
    "    bai = tf.Variable(tf.constant(0.1, shape=[attention_size]), name=\"bai\")\n",
    "    bbi = tf.Variable(tf.constant(0.1, shape=[1]), name=\"bbi\")\n",
    "    \n",
    "#     self.uid_a = tf.nn.relu(tf.nn.embedding_lookup(uidW, self.input_reiid))\n",
    "    i_j =tf.einsum('ajk,kl->ajl', tf.nn.relu(\n",
    "                tf.einsum('ajk,kl->ajl', h_drop_i, Wai) + tf.einsum('ajk,kl->ajl', uid_a, Wri) + bai),\n",
    "                                             Wpi)+bbi\n",
    "\n",
    "    i_a = tf.nn.softmax(i_j,1)  # none*len*1\n",
    "    \n",
    "    l2_loss += tf.nn.l2_loss(Wri)\n",
    "    l2_loss += tf.nn.l2_loss(Wai)\n",
    "    \n",
    "    return i_a,l2_loss\n",
    "\n",
    "def item_path(input_i, input_iid, input_reiid, uidW, dropout_keep_prob,review_num_i,item_num,item_vocab_size,\n",
    "              attention_size,embedding_size,embedding_id,n_latent,filter_sizes,num_filters,l2_loss,attention_scope,get_fea_scope):\n",
    "    with tf.name_scope(\"item_embedding\"):\n",
    "        W2 = tf.Variable(\n",
    "                tf.random_uniform([item_vocab_size, embedding_size], -1.0, 1.0),\n",
    "                name=\"W2\")\n",
    "        embedded_item = tf.nn.embedding_lookup(W2, input_i)\n",
    "        embedded_items = tf.expand_dims(embedded_item, -1)\n",
    "        \n",
    "    pooled_outputs_i = cnn_block(embedded_items,embedding_size,filter_sizes,num_filters,name='item_conv-maxpool')      \n",
    "    num_filters_total = num_filters * len(filter_sizes)\n",
    "    h_pool_i = tf.concat(pooled_outputs_i,axis=-1)\n",
    "    h_pool_flat_i = tf.reshape(h_pool_i, [-1, review_num_i, num_filters_total])\n",
    "        \n",
    "    with tf.name_scope(\"dropout\"):\n",
    "        h_drop_i = tf.nn.dropout(h_pool_flat_i, 1.0)\n",
    "            \n",
    "    with tf.name_scope(attention_scope):\n",
    "        uid_a = tf.nn.relu(tf.nn.embedding_lookup(uidW, input_reiid))\n",
    "        i_a,l2_loss = item_attetion_block(h_drop_i,uid_a,attention_size,embedding_id,num_filters_total,l2_loss)\n",
    "            \n",
    "    with tf.name_scope(\"add_reviews\"):\n",
    "        i_feas = tf.reduce_sum(tf.multiply(i_a, h_drop_i), 1)\n",
    "        i_feas = tf.nn.dropout(i_feas, dropout_keep_prob)\n",
    "           \n",
    "        \n",
    "    with tf.name_scope(get_fea_scope):\n",
    "        iidmf = tf.Variable(tf.random_uniform([item_num + 2, embedding_id], -0.1, 0.1), name=\"iidmf\")\n",
    "        iid = tf.nn.embedding_lookup(iidmf,input_iid)\n",
    "        iid = tf.reshape(iid,[-1,embedding_id])\n",
    "            \n",
    "        Wi = tf.Variable(\n",
    "                tf.random_uniform([num_filters_total, n_latent], -0.1, 0.1), name='Wi')\n",
    "        bi = tf.Variable(tf.constant(0.1, shape=[n_latent]), name=\"bi\")\n",
    "        i_feas = tf.matmul(i_feas, Wi) +iid+ bi\n",
    "\n",
    "    return i_a,i_feas,l2_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/narre.png\" width=600>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NARRE(object):\n",
    "    def __init__(\n",
    "            self, review_num_u, review_num_i, review_len_u, review_len_i, user_num, item_num, num_classes,\n",
    "            user_vocab_size, item_vocab_size, n_latent, embedding_id, attention_size,\n",
    "            embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
    "        self.input_u = tf.placeholder(tf.int32, [None, review_num_u, review_len_u], name=\"input_u\")\n",
    "        self.input_i = tf.placeholder(tf.int32, [None, review_num_i, review_len_i], name=\"input_i\")\n",
    "        self.input_reuid = tf.placeholder(tf.int32, [None, review_num_u], name='input_reuid')\n",
    "        self.input_reiid = tf.placeholder(tf.int32, [None, review_num_i], name='input_reuid')\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, 1], name=\"input_y\")\n",
    "        self.input_uid = tf.placeholder(tf.int32, [None, 1], name=\"input_uid\")\n",
    "        self.input_iid = tf.placeholder(tf.int32, [None, 1], name=\"input_iid\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "        self.drop0 = tf.placeholder(tf.float32, name=\"dropout0\")\n",
    "        iidW = tf.Variable(tf.random_uniform([item_num + 2, embedding_id], -0.1, 0.1), name=\"iidW\")\n",
    "        uidW = tf.Variable(tf.random_uniform([user_num + 2, embedding_id], -0.1, 0.1), name=\"uidW\")\n",
    "\n",
    "        l2_loss = tf.constant(0.0)\n",
    "        \n",
    "        \n",
    "        self.u_a,self.u_feas, l2_loss,attention_scope,get_fea_scope = user_path(self.input_u, self.input_uid, self.input_reuid, iidW, self.dropout_keep_prob,review_num_u,user_num,user_vocab_size,\n",
    "              attention_size,embedding_size,embedding_id,n_latent,filter_sizes,num_filters,l2_loss)\n",
    "\n",
    "        self.i_a,self.i_feas, l2_loss = item_path(self.input_i, self.input_iid, self.input_reiid, uidW, self.dropout_keep_prob,review_num_i,item_num,item_vocab_size,\n",
    "              attention_size,embedding_size,embedding_id,n_latent,filter_sizes,num_filters,l2_loss,attention_scope,get_fea_scope)\n",
    "\n",
    "\n",
    "        with tf.name_scope('ncf'):\n",
    "\n",
    "            self.FM = tf.multiply(self.u_feas, self.i_feas)\n",
    "            self.FM = tf.nn.relu(self.FM)\n",
    "\n",
    "            self.FM=tf.nn.dropout(self.FM,self.dropout_keep_prob)\n",
    "\n",
    "            Wmul=tf.Variable(\n",
    "                tf.random_uniform([n_latent, 1], -0.1, 0.1), name='wmul')\n",
    "\n",
    "            self.mul=tf.matmul(self.FM,Wmul)\n",
    "            self.score=tf.reduce_sum(self.mul,1,keep_dims=True)\n",
    "\n",
    "            self.uidW2 = tf.Variable(tf.constant(0.1, shape=[user_num + 2]), name=\"uidW2\")\n",
    "            self.iidW2 = tf.Variable(tf.constant(0.1, shape=[item_num + 2]), name=\"iidW2\")\n",
    "            self.u_bias = tf.gather(self.uidW2, self.input_uid)\n",
    "            self.i_bias = tf.gather(self.iidW2, self.input_iid)\n",
    "            self.Feature_bias = self.u_bias + self.i_bias\n",
    "\n",
    "            self.bised = tf.Variable(tf.constant(0.1), name='bias')\n",
    "\n",
    "            self.predictions = self.score + self.Feature_bias + self.bised\n",
    "\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.l2_loss(tf.subtract(self.predictions, self.input_y))\n",
    "\n",
    "            self.loss = losses + l2_reg_lambda * l2_loss\n",
    "\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            self.mae = tf.reduce_mean(tf.abs(tf.subtract(self.predictions, self.input_y)))\n",
    "            self.accuracy =tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(self.predictions, self.input_y))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2017)\n",
    "random_seed = 2017\n",
    "\n",
    "word2vec = 'NARRE-master/data/google.bin'\n",
    "valid_data = \"NARRE-master/data/music/music.test\"\n",
    "para_data = \"NARRE-master/data/music/music.para\"\n",
    "train_data = \"NARRE-master/data/music/music.train\"\n",
    "\n",
    "embedding_dim = 300\n",
    "filter_sizes = \"3\"\n",
    "num_filters = 100\n",
    "dropout_keep_prob = 0.5\n",
    "l2_reg_lambda = 0.001\n",
    "\n",
    "batch_size =  256\n",
    "num_epochs = 40\n",
    "\n",
    "allow_soft_placement = True\n",
    "log_device_placement = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/var/nvidia/vishal\n",
      "Loading data...\n",
      "CPU times: user 12.8 s, sys: 1.4 s, total: 14.2 s\n",
      "Wall time: 14.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%cd /var/nvidia/vishal/\n",
    "print(\"Loading data...\")\n",
    "pkl_file = open(para_data, 'rb')\n",
    "\n",
    "para = pickle.load(pkl_file)\n",
    "user_num = para['user_num']\n",
    "item_num = para['item_num']\n",
    "review_num_u = para['review_num_u']\n",
    "review_num_i = para['review_num_i']\n",
    "review_len_u = para['review_len_u']\n",
    "review_len_i = para['review_len_i']\n",
    "vocabulary_user = para['user_vocab']\n",
    "vocabulary_item = para['item_vocab']\n",
    "train_length = para['train_length']\n",
    "test_length = para['test_length']\n",
    "u_text = para['u_text']\n",
    "i_text = para['i_text']\n",
    "\n",
    "pkl_file = open(valid_data, 'rb')\n",
    "test_data = pickle.load(pkl_file)\n",
    "test_data = np.array(test_data)\n",
    "data_size_test = len(test_data)\n",
    "pkl_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_id = 1\n",
    "inv_vocabulary_user = {v:k for k,v in vocabulary_user.iteritems()}\n",
    "inv_vocabulary_item = {v:k for k,v in vocabulary_item.iteritems()}\n",
    "\n",
    "def get_text_from_review_ids(arr,vocab=inv_vocabulary_user):\n",
    "    review = ''\n",
    "    for a in arr:\n",
    "        if(a==pad_id):\n",
    "            return review.rstrip(\" \")\n",
    "        else:\n",
    "            review+=vocab[a]+\" \"\n",
    "    return review.rstrip(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dev_step_with_attention(u_batch, i_batch, uid, iid, reuid, reiid, y_batch, writer=None):\n",
    "    \"\"\"\n",
    "    Evaluates model on a dev set\n",
    "\n",
    "    \"\"\"\n",
    "    feed_dict = {\n",
    "        deep.input_u: u_batch,\n",
    "        deep.input_i: i_batch,\n",
    "        deep.input_y: y_batch,\n",
    "        deep.input_uid: uid,\n",
    "        deep.input_iid: iid,\n",
    "        deep.input_reuid: reuid,\n",
    "        deep.input_reiid: reiid,\n",
    "        deep.drop0: 1.0,\n",
    "        deep.dropout_keep_prob: 1.0\n",
    "    }\n",
    "    loss, accuracy, mae, u_a, i_a = sess.run(\n",
    "        [deep.loss, deep.accuracy, deep.mae,deep.u_a, deep.i_a,],\n",
    "        feed_dict)\n",
    "    return [loss, accuracy, mae, u_a, i_a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1021 20:53:31.858580 140155372729280 deprecation.py:506] From <ipython-input-3-ba0eb96b92fd>:41: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W1021 20:53:32.109607 140155372729280 deprecation.py:506] From <ipython-input-5-ff579f09b996>:39: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "W1021 20:53:32.290285 140155372729280 deprecation.py:323] From /dev/shm/vishal/vishal.Linux_debian_9_3.py27.keras2/lib/python2.7/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('instance: ', 1562, 1392, 5.0)\n",
      "('Reviews given by user: ', 1562, '/n')\n",
      "(0, 0.07464726, u'from all beegees albums i would say this one makes you the most curious once you purchase it songs like never say never again suddenly first of may sound of love and melody fair are incredible but the rest of the songs make it sound like they just sat down in their studio and whatever popped into their head they produced right away but some of the songs are still pretty good those three classical tunes i really don t listen to but not a bad cd for songs')\n",
      "(1, 0.36030912, u'my favorite songs on this cd happen to be the weirdest named ones birdie told me the earnest of being george and lemons never forget that nd one i don t have a slight idea wut it means but it has a cool beat to it birdie told me and lemons never forget are mid paced songs and they re both catchy the hit quot massachusetts quot is boring to me it just ain t my style but world is robin does great and that song maurice s song is with the sun in my eyes good tune but not much variety in it the change is made is ok but it seems like a hard attempt of a rock song really and sincerely is pretty ok it seems like all of their albums have one slow sad song by robin harry braff is the weird one on this cd and the chorus has an odd tune that keeps going up and down and somehow makes it ok to listen to daytime girl is short but good and the sun will shine is great i like the way they set up the chorus and outro that leads to the verses the album ends with horizontal a pretty nice little tune so not a bad album at all for their nd one')\n",
      "(2, 0.13825853, u'other than the boring quot in the summer of his years quot and the weird non beegees written quot such a shame quot the rest of this album is incredible amongst my favorites are the hit quot let there be love quot and the hard toned quot down to earth quot quot i started a joke quot is one of robin s all time most successful single hits and it s also awesome so is the other hard bassed chorus of quot i ve gotta get a message to you quot the weird lyrics of quot kitty can quot mean nothing next to its catchy tune quot kilburn towers quot has a beautiful melody and so does the album finale quot swan song quot quot i have decided to join the airforce quot is also a good song but why would he wanna join the airforce just to not be alone quot idea quot and quot indian gin and whisky dry quot have catchy tunes and i also love the barry gibb song quot when the swallows fly quot so for only their third album i think they had a tremendous success especially with those two songs')\n",
      "(3, 0.15218566, u'i love rap but most of the rap albums i have have only or great songs on it and the rest are okay or just stink a couple of examples are the great depression and love is pain even eminem s stan is not nearly as good as this here s my opinion on each of the songs not including the skits and the skitlike the kiss white america i love the verses of this song and the chorus is good too but maybe he could ve been a bit quieter on the volume business this song is outstanding and the h ll yeah parts by dr dre are amazing as well i also like the background tune for the chorus cleanin out my closet although he s pretty mean to his mother at some points of this song he probably doesn t mean it but anyway he actually can sing in this song i also love the background tune throughout the whole song square dance there are three parts of this song where he really changes his style of singing and puts a likingness into it that s when he s on not even on my radar oh no i won t leave no and gotta take some antibiotic the chorus is okay soldier this song sort of continues out of the violent kiss but anyway the chorus is too boring it doesn t have much excitement in it but the verses are good for eminem s fast singing say goodbye hollywood again the chorus is too boring but again the verses are good and the beat of the chorus is good too drips this song is sick but obie s chorus the main sick part is pretty good why is he saying that anyway the weird tuned chorus is also awesome and then eminem s part too without me the stan of this album this song is a big hit and lots of reviewers are using shady s back in a way in their title eminem adds a huge variety of voices and tunes in this song and there are also good sound effects the chorus is catchy sing for the moment the chorus is the only part of this song that i don t really like and i completely don t understand the maybe tomorrow the good lord will take you away part but the verses are pure awesome superman one of my favorites this song has such an awesome opening stupendous verses and the outstanding chorus you just cannot hate this song hallie s song eminem puts a tremendous effort into singing into this song but i m sorry it just wasn')\n",
      "(4, 0.26849267, u'oh man is this album hot especially tracks and what up gangsta is just what cent is made for and high all the time is just plain sweet that chorus is soo awesome then of course there s in da club which you gotta just love go shorty its ur birthday haha love it sorry anyway many men is also awesome i ll never get sick of that chorus back down is sort of way too hard beated and doesnt really match cent more like nappy roots style maybe speaking of style i also love like my style especially the background beat throughout the song blood hound is weird and song has eminem i think so does i also like if i cant do it my homie it cant be done and ima make the champagne bottle pop ima take it to the top to show i can make it stop baby sorry again heat is boring poor lil rich is boring p i m p sounds like some sorta hawaiin or jamaican song with that tune and the other two songs are also boring but the rest great buy it no matter what')\n",
      "('Reviews recieved on item: ', 1392, '/n')\n",
      "(0, array([0.28965685], dtype=float32), u'although i consider myself a gees fan i have to admit that robin is my least favourite vocalist of the group that said after liking a couple of the singles i decided to give the album a go on the plus side the songs are as strongly melodic as ever at least half the tracks here would have made good singles back in the day the production is a typically early s synthy drone that does tend to detract from the songs years on gibb s vocals however are an utter nightmare i don t know if the original lp had a lyric sheet but boy you certainly need one as rg s unearthly shrieks render most of the songs unintelligable it s too much of a not necessarily good thing as robin is given full reign to wreck almost every song with a grotesquely ott display of appalling singing finally i should also add that this cd really needs remastering as it s very soft sounding with precious little bass and far too much treble')\n",
      "(1, array([0.6039478], dtype=float32), u'this is the only solo album of the brothers gibb that i own and think this is a very good record juliet is one of my favorite songs of all time and the rest the record is good too')\n",
      "(2, array([0.10385968], dtype=float32), u'in robin gibb unamiacally left his brothers to record robin s reign an unfinished album redeemed only by a uk single called saved by the bell topped on the charts by honky tonk women go figure this current album released in contains help from the bee gees old band members alan kendall and dennis bryon and twin brother maurice it did well in europe and drowned in the then anti bee gees usa the first song juliet which went to in europe is a thumper falsetto riddled uptempto masterpiece still used in their concerts how old are you in and out of love kathy s gone and another lonely night in new york another hit are nearly as good from then on it goes downhill the drum beats become monotonous the lyrics banal and robin s high shrilled attempts at reaching the highest falsetto notes imaginable are mind numbing don t stop the night and i believe in miracles will send you running to the medicine cabinet for pain relievers considering that half of the songs are very good is enough reason to purchase this cd if that isn t enough reason then think about the fact that in probably less than a year you ll have to pay some scalping rare records bozo a fortune for it have a good time and watch that volume')\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    deep = NARRE(\n",
    "                review_num_u=review_num_u,\n",
    "                review_num_i=review_num_i,\n",
    "                review_len_u=review_len_u,\n",
    "                review_len_i=review_len_i,\n",
    "                user_num=user_num,\n",
    "                item_num=item_num,\n",
    "                num_classes=1,\n",
    "                user_vocab_size=len(vocabulary_user),\n",
    "                item_vocab_size=len(vocabulary_item),\n",
    "                embedding_size=embedding_dim,\n",
    "                embedding_id=32,\n",
    "                filter_sizes=list(map(int, filter_sizes.split(\",\"))),\n",
    "                num_filters=num_filters,\n",
    "                l2_reg_lambda=l2_reg_lambda,\n",
    "                attention_size=32,\n",
    "                n_latent=32)\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, \"narre_model/\")\n",
    "    ll_test = int(len(test_data) / batch_size) + 1\n",
    "    for batch_num in range(10,11):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size_test)\n",
    "        data_test = test_data[start_index:end_index]\n",
    "\n",
    "        userid_valid, itemid_valid, reuid, reiid, y_valid = zip(*data_test)\n",
    "        \n",
    "#         print(userid_valid[0][0],itemid_valid)\n",
    "        u_valid = []\n",
    "        i_valid = []\n",
    "        for i in range(len(userid_valid)):\n",
    "            u_valid.append(u_text[userid_valid[i][0]])\n",
    "            i_valid.append(i_text[itemid_valid[i][0]])\n",
    "        u_valid = np.array(u_valid)\n",
    "        i_valid = np.array(i_valid)\n",
    "\n",
    "        loss, accuracy, mae,u_a,i_a = dev_step_with_attention(u_valid, i_valid, userid_valid, itemid_valid, reuid, reiid,\n",
    "                                                           y_valid)\n",
    "        \n",
    "        print(\"instance: \",userid_valid[0][0],itemid_valid[0][0],y_valid[0][0])\n",
    "        print(\"Reviews given by user: \",userid_valid[0][0],\"/n\")\n",
    "        for i,arr in enumerate(u_text[userid_valid[0][0]]):\n",
    "            review = get_text_from_review_ids(arr,inv_vocabulary_user)\n",
    "            if(review):\n",
    "                print(i,u_a[0][i][0],review)\n",
    "        print(\"Reviews recieved on item: \",itemid_valid[0][0],\"/n\")\n",
    "        for i,arr in enumerate(i_text[itemid_valid[0][0]]):\n",
    "            review = get_text_from_review_ids(arr,inv_vocabulary_item)\n",
    "            if(review):\n",
    "                print(i,i_a[0][i],review)\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 90% of this code is from https://github.com/chenchongthu/NARRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Show attention score for users and items with less length of review"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
