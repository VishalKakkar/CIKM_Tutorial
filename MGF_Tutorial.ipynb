{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging,os\n",
    "\n",
    "logging.getLogger('tensorflow').disabled = True\n",
    "\n",
    "import warnings  \n",
    "warnings.filterwarnings('ignore')\n",
    "# warnings.filterwarnings(\"ignore\",category=FutureWarning)\n",
    "# warnings.filterwarnings(\"ignore\",category=Warning)\n",
    "# warnings.filterwarnings(\"ignore\",category=RuntimeWarning)\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# with warnings.catch_warnings():  \n",
    "#     warnings.filterwarnings(\"ignore\",category=FutureWarning)\n",
    "#     warnings.filterwarnings(\"ignore\",category=Warning)\n",
    "#     warnings.filterwarnings(\"ignore\",category=RuntimeWarning)\n",
    "#     warnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")\n",
    "#     import tensorflow as tf\n",
    "#     import numpy as np\n",
    "\n",
    "# import tensorflow as tf\n",
    "\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "from collections import defaultdict\n",
    "from reader import batch_review_normalize, PAD_INDEX, START_INDEX,DataReader, get_review_data\n",
    "from utils import load_glove, get_shape, count_parameters, load_vocabulary, decode_reviews, log_info\n",
    "from bleu import compute_bleu\n",
    "from rouge import rouge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/mgf_rp.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(self, user_emb, item_emb, num_layers=1):\n",
    "        with tf.variable_scope('features', reuse=tf.AUTO_REUSE):\n",
    "            features = tf.concat([user_emb, item_emb], axis=1)\n",
    "            for layer in range(num_layers):\n",
    "                w = tf.get_variable('w{}'.format(layer), [2 * self.F, 2 * self.F], initializer=self.weight_initializer)\n",
    "                b = tf.get_variable('b{}'.format(layer), [2 * self.F], initializer=self.const_initializer)\n",
    "                features = tf.matmul(features, w) + b\n",
    "                features = tf.nn.tanh(features, 'h{}'.format(layer))\n",
    "\n",
    "            return features\n",
    "        \n",
    "def build_rating_predictor(self):\n",
    "    features = self._get_features(self.user_emb, self.item_emb)\n",
    "\n",
    "    with tf.variable_scope('rating'):\n",
    "        rating_labels = tf.reshape(self.ratings, [-1, 1])\n",
    "        rating_preds = self.global_rating + tf.layers.dense(features, units=1, name='prediction')\n",
    "        self.rating_loss = tf.losses.mean_squared_error(rating_labels, rating_preds)\n",
    "\n",
    "        self.rating_preds = tf.clip_by_value(rating_preds, clip_value_min=1.0, clip_value_max=5.0)\n",
    "        self.mae, mae_update = tf.metrics.mean_absolute_error(rating_labels, self.rating_preds, name='metrics/MAE')\n",
    "        self.rmse, rmse_update = tf.metrics.root_mean_squared_error(rating_labels, self.rating_preds, name='metrics/RMSE')\n",
    "\n",
    "        metric_vars = tf.get_collection(tf.GraphKeys.LOCAL_VARIABLES, scope=\"rating/metrics\")\n",
    "        self.init_metrics = tf.variables_initializer(var_list=metric_vars)\n",
    "        self.update_metrics = tf.group([mae_update, rmse_update])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/mgf_att.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_layer(self, h, features, features_proj):\n",
    "        with tf.variable_scope('attention'):\n",
    "            L = get_shape(features)[1]\n",
    "\n",
    "            w = tf.get_variable('w', [self.C, self.D], initializer=self.weight_initializer)\n",
    "            b = tf.get_variable('b', [self.D], initializer=self.const_initializer)\n",
    "            w_att = tf.get_variable('w_att', [self.D, 1], initializer=self.weight_initializer)\n",
    "            b_att = tf.get_variable('b_att', [1], initializer=self.const_initializer)\n",
    "\n",
    "            h_att = tf.nn.tanh(features_proj + tf.expand_dims(tf.matmul(h, w), 1) + b)\n",
    "            out_att = tf.reshape(tf.matmul(tf.reshape(h_att, [-1, self.D]), w_att) + b_att, [-1, L])\n",
    "            alpha = tf.nn.softmax(out_att)\n",
    "            context = tf.reduce_sum(features * tf.expand_dims(alpha, 2), 1, name='context')\n",
    "            return context, alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/mgf_fusion.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fusion_gate(self, x, h, s_features, v_features):\n",
    "        with tf.variable_scope('fusion_gate'):\n",
    "            w_x = tf.get_variable('w_x', [self.W, 1], initializer=self.weight_initializer)\n",
    "            w_h = tf.get_variable('w_h', [self.C, 1], initializer=self.weight_initializer)\n",
    "            b = tf.get_variable('b', [1], initializer=self.const_initializer)\n",
    "            beta = tf.nn.sigmoid(tf.matmul(x, w_x) + tf.matmul(h, w_h) + b)  # (N, 1)\n",
    "            weighted_features = tf.multiply(beta, s_features) + tf.multiply((1. - beta), v_features)\n",
    "            return weighted_features, beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/mgf_nlp.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_lstm(self):\n",
    "        with tf.variable_scope('init_lstm'):\n",
    "            user_item_emb = tf.concat([self.user_emb, self.item_emb], axis=1)\n",
    "\n",
    "            w_h_ui = tf.get_variable('w_h_ui', [self.D, self.C], initializer=self.weight_initializer)\n",
    "            b_h = tf.get_variable('b_h', [self.C], initializer=self.const_initializer)\n",
    "            h = tf.matmul(user_item_emb, w_h_ui) + b_h\n",
    "\n",
    "            w_c_ui = tf.get_variable('w_c_ui', [self.D, self.C], initializer=self.weight_initializer)\n",
    "            b_c = tf.get_variable('b_c', [self.C], initializer=self.const_initializer)\n",
    "            c = tf.matmul(user_item_emb, w_c_ui) + b_c\n",
    "\n",
    "            h = tf.nn.tanh(h)\n",
    "            c = tf.nn.tanh(c)\n",
    "            return c, h\n",
    "\n",
    "def decode_lstm(self, x, h, context):\n",
    "        with tf.variable_scope('decode_lstm'):\n",
    "            w_h = tf.get_variable('w_h', [self.C, self.W], initializer=self.weight_initializer)\n",
    "            b_h = tf.get_variable('b_h', [self.W], initializer=self.const_initializer)\n",
    "            w_out = tf.get_variable('w_out', [self.W, self.V], initializer=self.weight_initializer)\n",
    "            b_out = tf.get_variable('b_out', [self.V], initializer=self.const_initializer)\n",
    "\n",
    "            h = tf.layers.dropout(h, self.dropout_rate, training=self.is_training)\n",
    "            h_logits = tf.matmul(h, w_h) + b_h\n",
    "\n",
    "            w_ctx2out = tf.get_variable('w_ctx2out', [get_shape(context)[1], self.W], initializer=self.weight_initializer)\n",
    "            h_logits += tf.matmul(context, w_ctx2out)\n",
    "\n",
    "            h_logits += x\n",
    "            h_logits = tf.nn.tanh(h_logits)\n",
    "\n",
    "            h_logits = tf.layers.dropout(h_logits, self.dropout_rate, training=self.is_training)\n",
    "            out_logits = tf.matmul(h_logits, w_out) + b_out\n",
    "            return out_logits\n",
    "\n",
    "def build_review_generator(self):\n",
    "        with tf.variable_scope('review', reuse=tf.AUTO_REUSE):\n",
    "            reviews_inputs = self.reviews[:, :self.T - 1]\n",
    "            reviews_emb = tf.nn.embedding_lookup(self.word_matrix, reviews_inputs)\n",
    "            reviews_labels = self.reviews[:, 1:]\n",
    "            mask = tf.to_float(tf.not_equal(reviews_labels, PAD_INDEX))\n",
    "\n",
    "            loss = 0.0\n",
    "\n",
    "            self.cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=self.C, name='LSTM_Cell')\n",
    "            c, h = self._init_lstm()\n",
    "\n",
    "            for t in range(self.T - 1):\n",
    "                x = reviews_emb[:, t, :]\n",
    "                visual_context, alpha = self._attention_layer(h, self.visual_features, self.visual_projection)\n",
    "                context, beta = self._fusion_gate(x, h, self.sentiment_features, visual_context)\n",
    "                cell_input = tf.concat([x, context], axis=1)\n",
    "                _, (c, h) = self.cell(inputs=cell_input, state=[c, h])\n",
    "\n",
    "                logits = self._decode_lstm(x, h, context)\n",
    "\n",
    "                loss += tf.reduce_sum(\n",
    "                  tf.nn.sparse_softmax_cross_entropy_with_logits(labels=reviews_labels[:, t], logits=logits) * mask[:, t])\n",
    "\n",
    "            self.review_loss = loss / tf.reduce_sum(mask)\n",
    "\n",
    "def build_review_sampler(self, max_decode_length):\n",
    "        with tf.variable_scope('review', reuse=tf.AUTO_REUSE):\n",
    "            sampled_word_list = []\n",
    "            beta_list = []\n",
    "            alpha_list = []\n",
    "\n",
    "            c, h = self._init_lstm()\n",
    "\n",
    "            batch_size = tf.shape(self.users)[0]\n",
    "            sampled_word = tf.fill([batch_size], START_INDEX)\n",
    "            for t in range(max_decode_length):\n",
    "                x = tf.nn.embedding_lookup(self.word_matrix, sampled_word)\n",
    "\n",
    "                visual_context, alpha = self._attention_layer(h, self.visual_features, self.visual_projection)\n",
    "                alpha_list.append(alpha)\n",
    "                context, beta = self._fusion_gate(x, h, self.sentiment_features, visual_context)\n",
    "                beta_list.append(beta)\n",
    "\n",
    "                cell_input = tf.concat([x, context], axis=1)\n",
    "                _, (c, h) = self.cell(inputs=cell_input, state=[c, h])\n",
    "\n",
    "                logits = self._decode_lstm(x, h, context)\n",
    "\n",
    "                sampled_word = tf.argmax(logits, 1)\n",
    "                sampled_word_list.append(sampled_word)\n",
    "\n",
    "            self.sampled_reviews = tf.transpose(tf.stack(sampled_word_list), (1, 0))  # (N, max_len)\n",
    "            self.alphas = tf.transpose(tf.stack(alpha_list), (1, 0, 2))  # (N, T, L)\n",
    "            self.betas = tf.transpose(tf.squeeze(tf.stack(beta_list), axis=2), (1, 0))  # (N, T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/arch.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "\n",
    "    def __init__(self, total_users, total_items, global_rating,\n",
    "               num_factors, img_dims, vocab_size,\n",
    "               word_dim, lstm_dim, max_length, dropout_rate):\n",
    "        \n",
    "        self.total_users = total_users\n",
    "        self.total_items = total_items\n",
    "        self.global_rating = global_rating\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.F = num_factors\n",
    "        self.L = img_dims[0]\n",
    "        self.D = img_dims[1]\n",
    "        self.V = vocab_size\n",
    "        self.W = word_dim\n",
    "        self.C = lstm_dim\n",
    "        self.T = max_length\n",
    "        Model._get_features = get_features\n",
    "        Model._build_rating_predictor = build_rating_predictor\n",
    "        Model._attention_layer = attention_layer\n",
    "        Model._fusion_gate = fusion_gate\n",
    "        Model._init_lstm = init_lstm\n",
    "        Model._decode_lstm = decode_lstm\n",
    "        Model._build_review_generator = build_review_generator\n",
    "        Model._build_review_sampler = build_review_sampler\n",
    "        \n",
    "        \n",
    "        self.weight_initializer = tf.contrib.layers.xavier_initializer()\n",
    "        self.const_initializer = tf.zeros_initializer()\n",
    "\n",
    "        self.users = tf.placeholder(tf.int32, shape=[None])\n",
    "        self.items = tf.placeholder(tf.int32, shape=[None])\n",
    "        self.ratings = tf.placeholder(tf.float32, shape=[None])\n",
    "        self.images = tf.placeholder(tf.float32, shape=[None, self.L, self.D])\n",
    "        self.reviews = tf.placeholder(tf.int32, shape=[None, None])\n",
    "        self.is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "        self._init_embeddings()\n",
    "\n",
    "        self.user_emb = tf.nn.embedding_lookup(self.user_matrix, self.users)\n",
    "        self.item_emb = tf.nn.embedding_lookup(self.item_matrix, self.items)\n",
    "\n",
    "        self.sentiment_features = self._get_features(self.user_emb, self.item_emb)\n",
    "        self.sentiment_features = self._batch_norm(self.sentiment_features, name='review/sentiment')\n",
    "        self.visual_features = self._batch_norm(self.images, name='review/visual')\n",
    "        self.visual_projection = self._visual_projection(self.visual_features)\n",
    "\n",
    "        self._build_rating_predictor()\n",
    "        self._build_review_generator()\n",
    "        self._build_review_sampler(max_decode_length=self.T)\n",
    "\n",
    "    def _init_embeddings(self):\n",
    "        self.user_matrix = tf.get_variable(\n",
    "          name='user_matrix',\n",
    "          shape=[self.total_users, self.F],\n",
    "          initializer=self.weight_initializer,\n",
    "          dtype=tf.float32\n",
    "        )\n",
    "\n",
    "        self.item_matrix = tf.get_variable(\n",
    "          name='item_matrix',\n",
    "          shape=[self.total_items, self.F],\n",
    "          initializer=self.weight_initializer,\n",
    "          dtype=tf.float32\n",
    "        )\n",
    "\n",
    "        self.word_matrix = tf.get_variable(\n",
    "          name='word_matrix',\n",
    "          shape=[self.V, self.W],\n",
    "          initializer=tf.constant_initializer(load_glove(self.V, self.W)),\n",
    "          dtype=tf.float32\n",
    "        )\n",
    "\n",
    "    def _batch_norm(self, x, name=None):\n",
    "        return tf.contrib.layers.batch_norm(inputs=x,\n",
    "                                            decay=0.95,\n",
    "                                            center=True,\n",
    "                                            scale=True,\n",
    "                                            is_training=self.is_training,\n",
    "                                            updates_collections=None,\n",
    "                                            scope=(name + '_batch_norm'))\n",
    "    \n",
    "\n",
    "    def _visual_projection(self, features):\n",
    "        with tf.variable_scope('review/visual_projection'):\n",
    "            w = tf.get_variable('w', [self.D, self.D], initializer=self.weight_initializer)\n",
    "            features_flat = tf.reshape(features, [-1, self.D])\n",
    "            features_proj = tf.matmul(features_flat, w)\n",
    "            features_proj = tf.reshape(features_proj, [-1, self.L, self.D])\n",
    "            return features_proj\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    def feed_dict(self, users, items, ratings=None, images=None, reviews=None, is_training=False):\n",
    "        fd = {\n",
    "          self.users: users,\n",
    "          self.items: items,\n",
    "          self.is_training: is_training\n",
    "        }\n",
    "        if ratings is not None:\n",
    "            fd[self.ratings] = ratings\n",
    "        if images is not None:\n",
    "            fd[self.images] = images\n",
    "        if reviews is not None:\n",
    "            fd[self.reviews] = batch_review_normalize(reviews, self.T)\n",
    "\n",
    "        return fd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "\n",
    "tf.flags.DEFINE_string(\"data_dir\", \"data\",\n",
    "                       \"\"\"Path to the data directory\"\"\")\n",
    "\n",
    "tf.flags.DEFINE_float(\"learning_rate\", 0.001,\n",
    "                      \"\"\"Learning rate (default: 3e-4)\"\"\")\n",
    "tf.flags.DEFINE_float(\"dropout_rate\", 0.2,\n",
    "                      \"\"\"Probability of dropping neurons (default: 0.2)\"\"\")\n",
    "tf.flags.DEFINE_float(\"lambda_reg\", 1e-4,\n",
    "                      \"\"\"Lambda hyper-parameter for regularization (default: 1e-4)\"\"\")\n",
    "\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 20,\n",
    "                        \"\"\"Number of training epochs (default: 20)\"\"\")\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 256,\n",
    "                        \"\"\"Batch size of reviews (default: 64)\"\"\")\n",
    "tf.flags.DEFINE_integer(\"num_factors\", 256,\n",
    "                        \"\"\"Number of latent factors for users/items (default: 256)\"\"\")\n",
    "tf.flags.DEFINE_integer(\"word_dim\", 200,\n",
    "                        \"\"\"Word embedding dimensions (default: 200)\"\"\")\n",
    "tf.flags.DEFINE_integer(\"lstm_dim\", 256,\n",
    "                        \"\"\"LSTM hidden state dimensions (default: 256)\"\"\")\n",
    "tf.flags.DEFINE_integer(\"max_length\", 20,\n",
    "                        \"\"\"Maximum length of reviews to be generated (default: 20)\"\"\")\n",
    "tf.flags.DEFINE_integer(\"display_step\", 10,\n",
    "                        \"\"\"Display info after number of steps (default: 10)\"\"\")\n",
    "\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True,\n",
    "                        \"\"\"Allow device soft device placement\"\"\")\n",
    "\n",
    "\n",
    "FLAGS = tf.flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_scope_rating(var_name):\n",
    "    for name in ['user', 'item', 'features', 'rating']:\n",
    "        if name in var_name:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def check_scope_review(var_name):\n",
    "    for name in ['user', 'item', 'features', 'review']:\n",
    "        if name in var_name:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def train_fn(model):\n",
    "    global_step = tf.train.create_global_step()\n",
    "\n",
    "    trainable_vars = tf.trainable_variables()\n",
    "    count_parameters(trainable_vars)\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(FLAGS.learning_rate)\n",
    "\n",
    "    rating_l2_loss = tf.add_n([tf.nn.l2_loss(v) for v in trainable_vars\n",
    "                                 if check_scope_rating(v.name) and 'bias' not in v.name])\n",
    "    model.rating_loss = model.rating_loss + FLAGS.lambda_reg * rating_l2_loss\n",
    "\n",
    "    review_l2_loss = tf.add_n([tf.nn.l2_loss(v) for v in trainable_vars\n",
    "                                 if check_scope_review(v.name) and 'bias' not in v.name])\n",
    "    model.review_loss = model.review_loss + FLAGS.lambda_reg * review_l2_loss\n",
    "\n",
    "    update_rating = optimizer.minimize(model.rating_loss, name='update_rating', global_step=global_step)\n",
    "    update_review = optimizer.minimize(model.review_loss, name='update_review')\n",
    "\n",
    "    return update_rating, update_review, global_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading vocabulary: data/vocab.pkl\n",
      "Reading data: data/users.txt\n",
      "Reading data: data/items.txt\n",
      "Total users: 2909, total items: 2726\n",
      "Reading data: data/train.pkl\n",
      "Reading data: data/test.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 4/220 [00:00<00:06, 34.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global rating: 3.97\n",
      "Reading image id_to_idx: data/train.id_to_idx.pkl\n",
      "Reading image features: data/img_feats/train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 220/220 [00:05<00:00, 37.41it/s]\n",
      "  7%|▋         | 4/55 [00:00<00:01, 39.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading image id_to_idx: data/test.id_to_idx.pkl\n",
      "Reading image features: data/img_feats/test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [00:01<00:00, 37.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained word embeddings\n",
      "Total 400000 word vectors in Glove 6B 200d.\n",
      "Number of OOV words: 915\n"
     ]
    }
   ],
   "source": [
    "vocab = load_vocabulary(FLAGS.data_dir)\n",
    "data_reader = DataReader(FLAGS.data_dir)\n",
    "\n",
    "model = Model(total_users=data_reader.total_users, total_items=data_reader.total_items,\n",
    "                global_rating=data_reader.global_rating, num_factors=FLAGS.num_factors,\n",
    "                img_dims=[196, 512], vocab_size=len(vocab), word_dim=FLAGS.word_dim,\n",
    "                lstm_dim=FLAGS.lstm_dim, max_length=FLAGS.max_length, dropout_rate=FLAGS.dropout_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "user_matrix:0                                                                        744704 params\n",
      "----------------------------------------------------------------------------------------------------\n",
      "item_matrix:0                                                                        697856 params\n",
      "----------------------------------------------------------------------------------------------------\n",
      "word_matrix:0                                                                       2669200 params\n",
      "----------------------------------------------------------------------------------------------------\n",
      "features/w0:0                                                                        262144 params\n",
      "----------------------------------------------------------------------------------------------------\n",
      "features/b0:0                                                                           512 params\n",
      "----------------------------------------------------------------------------------------------------\n",
      "review/sentiment_batch_norm/beta:0                                                      512 params\n",
      "----------------------------------------------------------------------------------------------------\n",
      "review/sentiment_batch_norm/gamma:0                                                     512 params\n",
      "----------------------------------------------------------------------------------------------------\n",
      "review/visual_batch_norm/beta:0                                                         512 params\n",
      "----------------------------------------------------------------------------------------------------\n",
      "review/visual_batch_norm/gamma:0                                                        512 params\n",
      "----------------------------------------------------------------------------------------------------\n",
      "review/visual_projection/w:0                                                         262144 params\n",
      "----------------------------------------------------------------------------------------------------\n",
      "rating/prediction/kernel:0                                                              512 params\n",
      "----------------------------------------------------------------------------------------------------\n",
      "rating/prediction/bias:0                                                                  1 params\n",
      "----------------------------------------------------------------------------------------------------\n",
      "review/init_lstm/w_h_ui:0                                                            131072 params\n",
      "----------------------------------------------------------------------------------------------------\n",
      "review/init_lstm/b_h:0                                                                  256 params\n",
      "----------------------------------------------------------------------------------------------------\n",
      "review/init_lstm/w_c_ui:0                                                            131072 params\n",
      "----------------------------------------------------------------------------------------------------\n",
      "review/init_lstm/b_c:0                                                                  256 params\n",
      "----------------------------------------------------------------------------------------------------\n",
      "review/attention/w:0                                                                 131072 params\n",
      "----------------------------------------------------------------------------------------------------\n",
      "review/attention/b:0                                                                    512 params\n",
      "----------------------------------------------------------------------------------------------------\n",
      "review/attention/w_att:0                                                                512 params\n",
      "----------------------------------------------------------------------------------------------------\n",
      "review/attention/b_att:0                                                                  1 params\n",
      "----------------------------------------------------------------------------------------------------\n",
      "review/fusion_gate/w_x:0                                                                200 params\n",
      "----------------------------------------------------------------------------------------------------\n",
      "review/fusion_gate/w_h:0                                                                256 params\n",
      "----------------------------------------------------------------------------------------------------\n",
      "review/fusion_gate/b:0                                                                    1 params\n",
      "----------------------------------------------------------------------------------------------------\n",
      "review/LSTM_Cell/kernel:0                                                            991232 params\n",
      "----------------------------------------------------------------------------------------------------\n",
      "review/LSTM_Cell/bias:0                                                                1024 params\n",
      "----------------------------------------------------------------------------------------------------\n",
      "review/decode_lstm/w_h:0                                                              51200 params\n",
      "----------------------------------------------------------------------------------------------------\n",
      "review/decode_lstm/b_h:0                                                                200 params\n",
      "----------------------------------------------------------------------------------------------------\n",
      "review/decode_lstm/w_out:0                                                          2669200 params\n",
      "----------------------------------------------------------------------------------------------------\n",
      "review/decode_lstm/b_out:0                                                            13346 params\n",
      "----------------------------------------------------------------------------------------------------\n",
      "review/decode_lstm/w_ctx2out:0                                                       102400 params\n",
      "----------------------------------------------------------------------------------------------------\n",
      "====================================================================================================\n",
      "Total trainable parameters: 8862933\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "update_rating, update_review, global_step = train_fn(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_run(sess,log_file,batch_size=64):\n",
    "        review_gen_corpus = defaultdict(list)\n",
    "        review_ref_corpus = defaultdict(list)\n",
    "\n",
    "        photo_bleu_scores = defaultdict(list)\n",
    "        photo_rouge_scores = defaultdict(list)\n",
    "\n",
    "        review_bleu_scores = defaultdict(list)\n",
    "        review_rouge_scores = defaultdict(list)\n",
    "        \n",
    "        sess.run(model.init_metrics)\n",
    "        test_step = 0\n",
    "        for users, items, ratings in data_reader.read_test_set(batch_size, rating_only=True):\n",
    "            test_step += 1\n",
    "\n",
    "            fd = model.feed_dict(users, items, ratings)\n",
    "            sess.run(model.update_metrics, feed_dict=fd)\n",
    "\n",
    "            review_users, review_items, review_ratings, photo_ids, reviews = get_review_data(users, items, ratings,\n",
    "                                                                                             data_reader.test_review)\n",
    "            img_idx = [data_reader.test_id2idx[photo_id] for photo_id in photo_ids]\n",
    "            images = data_reader.test_img_features[img_idx]\n",
    "\n",
    "            fd = model.feed_dict(users=review_users, items=review_items, images=images)\n",
    "            _reviews, _alphas, _betas = sess.run([model.sampled_reviews, model.alphas, model.betas], feed_dict=fd)\n",
    "\n",
    "            gen_reviews = decode_reviews(_reviews, vocab)\n",
    "            ref_reviews = [decode_reviews(batch_review_normalize(ref), vocab) for ref in reviews]\n",
    "\n",
    "            for user, item, gen, refs,ratings,photo in zip(review_users, review_items, gen_reviews, ref_reviews,review_ratings,photo_ids):\n",
    "#                 print(\"review_users\",user)\n",
    "#                 print(\"review_items\",item)\n",
    "#                 print(\"ref_reviews\",refs)\n",
    "#                 print(\"gen_reviews\",gen)\n",
    "#                 print(\"rating\",ratings)\n",
    "#                 print(\"photo_id\",photo)\n",
    "                review_gen_corpus[(user, item)].append(gen)\n",
    "                review_ref_corpus[(user, item)] += refs\n",
    "\n",
    "                bleu_scores = compute_bleu([refs], [gen], max_order=4, smooth=True)\n",
    "                for order, score in bleu_scores.items():\n",
    "                    photo_bleu_scores[order].append(score)\n",
    "\n",
    "                rouge_scores = rouge([gen], refs)\n",
    "                for metric, score in rouge_scores.items():\n",
    "                    photo_rouge_scores[metric].append(score)\n",
    "            \n",
    "            break;\n",
    "                    \n",
    "        _mae, _rmse = sess.run([model.mae, model.rmse])\n",
    "        log_info(log_file, '\\nRating prediction results: MAE={:.3f}, RMSE={:.3f}'.format(_mae, _rmse))\n",
    "        log_info(log_file, '\\nReview generation results:')\n",
    "        log_info(log_file, '- Photo level: BLEU-scores = {:.2f}, {:.2f}, {:.2f}, {:.2f}'.format(\n",
    "            np.array(photo_bleu_scores[1]).mean() * 100, np.array(photo_bleu_scores[2]).mean() * 100,\n",
    "            np.array(photo_bleu_scores[3]).mean() * 100, np.array(photo_bleu_scores[4]).mean() * 100))\n",
    "        \n",
    "        for user_item, gen_reviews in review_gen_corpus.items():\n",
    "            references = [list(ref) for ref in set(tuple(ref) for ref in review_ref_corpus[user_item])]\n",
    "            user_item_bleu_scores = defaultdict(list)\n",
    "            for gen in gen_reviews:\n",
    "#                 print(\"user_item\",user_item)\n",
    "#                 print(\"references\",references)\n",
    "#                 print(\"gen\",gen)\n",
    "                bleu_scores = compute_bleu([references], [gen], max_order=4, smooth=True)\n",
    "                for order, score in bleu_scores.items():\n",
    "                    user_item_bleu_scores[order].append(score)\n",
    "            for order, scores in user_item_bleu_scores.items():\n",
    "                  review_bleu_scores[order].append(np.array(scores).mean())\n",
    "\n",
    "            user_item_rouge_scores = defaultdict(list)\n",
    "            for gen in gen_reviews:\n",
    "                rouge_scores = rouge([gen], references)\n",
    "                for metric, score in rouge_scores.items():\n",
    "                    user_item_rouge_scores[metric].append(score)\n",
    "            for metric, scores in user_item_rouge_scores.items():\n",
    "                review_rouge_scores[metric].append(np.array(scores).mean())\n",
    "                \n",
    "        log_info(log_file, '- Review level: BLEU-scores = {:.2f}, {:.2f}, {:.2f}, {:.2f}'.format(\n",
    "            np.array(review_bleu_scores[1]).mean() * 100, np.array(review_bleu_scores[2]).mean() * 100,\n",
    "            np.array(review_bleu_scores[3]).mean() * 100, np.array(review_bleu_scores[4]).mean() * 100))\n",
    "        \n",
    "        for metric in ['rouge_1', 'rouge_2', 'rouge_l']:\n",
    "            log_info(log_file, '- Photo level: {} = {:.2f}, {:.2f}, {:.2f}'.format(\n",
    "              metric,\n",
    "              np.array(photo_rouge_scores['{}/p_score'.format(metric)]).mean() * 100,\n",
    "              np.array(photo_rouge_scores['{}/r_score'.format(metric)]).mean() * 100,\n",
    "              np.array(photo_rouge_scores['{}/f_score'.format(metric)]).mean() * 100))\n",
    "            log_info(log_file, '- Review level: {} = {:.2f}, {:.2f}, {:.2f}'.format(\n",
    "              metric,\n",
    "              np.array(review_rouge_scores['{}/p_score'.format(metric)]).mean() * 100,\n",
    "              np.array(review_rouge_scores['{}/r_score'.format(metric)]).mean() * 100,\n",
    "              np.array(review_rouge_scores['{}/f_score'.format(metric)]).mean() * 100))\n",
    "\n",
    "        log_info(log_file, '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading model parameters from %s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|          | 0/55 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 55/55 [01:22<00:00,  1.51s/it, rating_loss=0.518, review_loss=4.48]\n",
      "Training:   0%|          | 0/55 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 55/55 [01:15<00:00,  1.38s/it, rating_loss=0.509, review_loss=4.44]\n",
      "Training:   0%|          | 0/55 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 3/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 55/55 [01:15<00:00,  1.38s/it, rating_loss=0.521, review_loss=4.44]\n",
      "Training:   0%|          | 0/55 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 4/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 55/55 [01:15<00:00,  1.38s/it, rating_loss=0.513, review_loss=4.41]\n",
      "Training:   0%|          | 0/55 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 5/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 55/55 [01:15<00:00,  1.38s/it, rating_loss=0.521, review_loss=4.4] \n"
     ]
    }
   ],
   "source": [
    "log_file = open('log.txt', 'w')\n",
    "config = tf.ConfigProto(allow_soft_placement=FLAGS.allow_soft_placement)\n",
    "config.gpu_options.allow_growth = True\n",
    "with tf.Session(config=config) as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    ckpt = tf.train.get_checkpoint_state('./model_dir')\n",
    "    if ckpt:\n",
    "        print(\"Reading model parameters from %s\".format(ckpt.model_checkpoint_path))\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        print(\"Creating model with fresh parameters.\")\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(1, 5 + 1):\n",
    "        log_info(log_file, \"\\nEpoch: {}/{}\".format(epoch, FLAGS.num_epochs))\n",
    "        count = 0\n",
    "        sum_rating_loss = 0\n",
    "        sum_review_loss = 0\n",
    "        for users, items, ratings in data_reader.read_train_set(FLAGS.batch_size, rating_only=True):\n",
    "            count += 1\n",
    "            fd = model.feed_dict(users=users, items=items, ratings=ratings, is_training=True)\n",
    "            _step, _, _rating_loss = sess.run([global_step, update_rating, model.rating_loss], feed_dict=fd)\n",
    "            sum_rating_loss += _rating_loss\n",
    "\n",
    "            review_users, review_items, _, photo_ids, reviews = get_review_data(users, items, ratings,\n",
    "                                                                                data_reader.train_review)\n",
    "            img_idx = [data_reader.train_id2idx[photo_id] for photo_id in photo_ids]\n",
    "            images = data_reader.train_img_features[img_idx]\n",
    "\n",
    "            fd = model.feed_dict(users=review_users, items=review_items, images=images,\n",
    "                                 reviews=reviews, is_training=True)\n",
    "            _, _review_loss = sess.run([update_review, model.review_loss], feed_dict=fd)\n",
    "            sum_review_loss += _review_loss\n",
    "\n",
    "            if _step % FLAGS.display_step == 0:\n",
    "                data_reader.iter.set_postfix(rating_loss=(sum_rating_loss / count),review_loss=(sum_review_loss / count))\n",
    "        \n",
    "        checkpoint_path = os.path.join('./model_dir', \"mrg.ckpt\")\n",
    "        saver.save(sess, checkpoint_path)\n",
    "        \n",
    "        # Testing\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading model parameters from %s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Testing:   0%|          | 0/1751 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rating prediction results: MAE=1.651, RMSE=1.657\n",
      "\n",
      "Review generation results:\n",
      "- Photo level: BLEU-scores = 27.64, 16.27, 14.30, 13.90\n",
      "user_item (907, 754)\n",
      "references [['their', 'macarons', '?'], ['we', 'had', 'four', 'flavors', ':', 'lemon', ',', 'raspberry', ',', 'coffee', ',', 'and', 'chocolate', '.'], ['they', 'neither', 'smelled', 'nor', 'tasted', 'like', 'chocolate', 'and', 'coffee', '.'], ['the', 'lemon', 'and', 'raspberry', 'flavors', 'were', 'on', 'par', '.'], ['i', 'could', 'not', 'say', 'the', 'same', 'for', 'the', 'coffee', 'and', 'chocolate', 'flavors', 'though', '.']]\n",
      "gen ['the', 'chocolate', 'cake', 'was', 'a', 'little', 'too', 'much', '.']\n",
      "user_item (735, 1490)\n",
      "references [['while', 'we', 'waited', 'for', 'pizza', ',', 'we', '<UNK>', 'on', 'some', 'delicious', 'appetizers', '.'], ['pizza', 'was', 'delicious', 'even', 'leftovers', 'reheated', 'a', 'few', 'days', 'later', '.']]\n",
      "gen ['i', 'ordered', 'the', 'chicken', 'and', 'chicken', 'sandwich', '.']\n",
      "- Review level: BLEU-scores = 27.64, 16.27, 14.30, 13.90\n",
      "- Photo level: rouge_1 = 18.25, 12.50, 14.79\n",
      "- Review level: rouge_1 = 7.14, 4.17, 5.26\n",
      "- Photo level: rouge_2 = 0.00, 0.00, 0.00\n",
      "- Review level: rouge_2 = 0.00, 0.00, 0.00\n",
      "- Photo level: rouge_l = 17.36, 10.99, 12.28\n",
      "- Review level: rouge_l = 6.25, 3.85, 4.30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_file = open('log.txt', 'w')\n",
    "with tf.Session(config=config) as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    ckpt = tf.train.get_checkpoint_state('./model_dir')\n",
    "    if ckpt:\n",
    "        print(\"Reading model parameters from %s\".format(ckpt.model_checkpoint_path))\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        print(\"No Checkpoint Found Exiting\")\n",
    "    test_run(sess,log_file,256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 90% of this code is from https://github.com/PreferredAI/mrg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
