{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys,time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    " \n",
    "DATA_DIR = \"/var/nvidia/vishal/A3NCF-master/python/data/\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_feature(dataset, dimension, name):\n",
    "    \"\"\" read user and item features into dictionary.\"\"\"\n",
    "    entity_feature = {}\n",
    "    with open(os.path.join(DATA_DIR, dataset+\".\"+str(dimension)+\".\"+name+\".theta\")) as entity_file:\n",
    "        for entity_info in entity_file:\n",
    "            line = entity_info.strip().split(',')\n",
    "            entity = [float(i) for i in line[1:]]\n",
    "            entity_feature[int(line[0])] = np.array(entity, dtype=np.float32)\n",
    "    return entity_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(dataset='Baby', dimension=5, batch_size=128, is_training=True):\n",
    "    user_feature = read_feature(dataset, dimension, 'user')\n",
    "    item_feature = read_feature(dataset, dimension, 'item')\n",
    "    user_size = len(user_feature)\n",
    "    item_size = len(item_feature)\n",
    "\n",
    "    split = 'train' if is_training else 'test'\n",
    "    data = pd.read_csv(os.path.join(DATA_DIR, dataset+\".{}.dat\".format(split)), sep='\\t', header=None,names=['user', 'item', 'rating'],dtype={'rating': np.float32})\n",
    "\n",
    "    ######################## ADD FEATURES #####################\n",
    "    user_feature_col, item_feature_col = [], []\n",
    "    for i in range(len(data)):\n",
    "        user_feature_col.append(user_feature[data['user'][i]])\n",
    "        item_feature_col.append(item_feature[data['item'][i]])\n",
    "    # data['user_feature'] = user_feature_col\n",
    "    # data['item_feature'] = item_feature_col\n",
    "\n",
    "    data_dict = dict([('user', data['user'].values),\n",
    "                ('item', data['item'].values),\n",
    "                ('rating', data['rating'].values),\n",
    "                ('user_feature', np.array(user_feature_col)),\n",
    "                ('item_feature', np.array(item_feature_col))])\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(data_dict)\n",
    "    if is_training:\n",
    "        dataset = dataset.shuffle(10000)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    return dataset, user_size, item_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/A3ncf.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttNCF(object):\n",
    "    def __init__(self, user_size, item_size, embed_size, iterator,\n",
    "                 activation_func, lr, optim, regularizer, dropout, is_training):\n",
    "        self.user_size = user_size\n",
    "        self.item_size = item_size\n",
    "        self.embed_size = embed_size\n",
    "        self.iterator = iterator\n",
    "        self.activation_func = activation_func\n",
    "        self.lr = lr\n",
    "        self.optim = optim\n",
    "        self.regularizer_rate = regularizer\n",
    "        self.dropout = dropout\n",
    "        self.is_training = is_training\n",
    "\n",
    "    def get_data(self):\n",
    "        \"\"\" Obtain the input data from tensorflow iterator.\"\"\"\n",
    "        sample = self.iterator.get_next()\n",
    "\n",
    "        self.user = sample['user']\n",
    "        self.item = sample['item']\n",
    "        self.user_feature = sample['user_feature']\n",
    "        self.item_feature = sample['item_feature']\n",
    "        self.rating = sample['rating']\n",
    "\n",
    "    def inference(self):\n",
    "        self.regularizer = tf.contrib.layers.l2_regularizer(self.regularizer_rate)\n",
    "        \n",
    "        if self.activation_func == 'ReLU':\n",
    "            self.activation_func = tf.nn.relu\n",
    "        elif self.activation_func == 'Leaky_ReLU':\n",
    "            self.activation_func = tf.nn.leaky_relu\n",
    "        elif self.activation_func == 'ELU':\n",
    "            self.activation_func = tf.nn.elu\n",
    "\n",
    "        if self.optim == 'SGD':\n",
    "            self.optimizer = tf.train.GradientDescentOptimizer(self.lr, name='SGD')\n",
    "        elif self.optim == 'RMSProp':\n",
    "            self.optimizer = tf.train.RMSPropOptimizer(self.lr, decay=0.9, momentum=0.0, name='RMSProp')\n",
    "        elif self.optim == 'Adam':\n",
    "            self.optimizer = tf.train.AdamOptimizer(self.lr, name='Adam')\n",
    "\n",
    "    def create_model(self):\n",
    "        \"\"\" Create model from scratch. \"\"\"\n",
    "        with tf.name_scope(\"input\"):\n",
    "            self.user_embedding = tf.get_variable(\"user_embed\", [self.user_size, self.embed_size], dtype=tf.float32)\n",
    "            self.item_embedding = tf.get_variable(\"item_embed\", [self.item_size, self.embed_size], dtype=tf.float32)\n",
    "            self.user_embed = tf.nn.embedding_lookup(self.user_embedding, self.user)\n",
    "            self.item_embed = tf.nn.embedding_lookup(self.user_embedding, self.item)\n",
    "        with tf.name_scope(\"fusion\"):\n",
    "            self.user_fusion_add = self.user_embed + self.user_feature\n",
    "            self.item_fusion_add = self.item_embed + self.item_feature\n",
    "\n",
    "            self.user_fusion = tf.layers.dense(inputs=self.user_fusion_add,units=self.embed_size,activation=self.activation_func,\n",
    "                                               kernel_regularizer=self.regularizer,name='user_fusion')\n",
    "            self.item_fusion = tf.layers.dense(inputs=self.item_fusion_add,units=self.embed_size,activation=self.activation_func,\n",
    "                                               kernel_regularizer=self.regularizer,name='item_fusion')\n",
    "\n",
    "        with tf.name_scope(\"attention\"):\n",
    "            self.feature_all = tf.concat([self.user_feature, self.item_feature,self.user_fusion, self.item_fusion], -1)\n",
    "            self.att_layer1 = tf.layers.dense(inputs=self.feature_all,units=1,activation=self.activation_func,\n",
    "                                              kernel_regularizer=self.regularizer,name='att_layer1')\n",
    "            self.att_layer2 = tf.layers.dense(inputs=self.att_layer1,units=self.embed_size,activation=self.activation_func,\n",
    "                                              kernel_regularizer=self.regularizer,name='att_layer2')\n",
    "            self.att_weights = tf.nn.softmax(self.att_layer2, axis=-1, name='att_softmax')\n",
    "\n",
    "        with tf.name_scope(\"prediction\"):\n",
    "            self.interact = self.att_weights*self.user_fusion*self.item_fusion\n",
    "            self.interact1 = tf.layers.dense(inputs=self.interact,units=self.embed_size,activation=self.activation_func,\n",
    "                                             kernel_regularizer=self.regularizer,name='interact1')\n",
    "            self.interact1 = tf.nn.dropout(self.interact1, self.dropout)\n",
    "            self.prediction = tf.layers.dense(inputs=self.interact,units=1,activation=None,\n",
    "                                              kernel_regularizer=self.regularizer,name='prediction')\n",
    "            self.prediction = tf.reshape(self.prediction, [-1])\n",
    "\n",
    "    def loss_func(self):\n",
    "        reg = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "        reg_loss = tf.contrib.layers.apply_regularization(self.regularizer, reg)\n",
    "        mse_loss = tf.losses.mean_squared_error(self.rating, self.prediction)\n",
    "        self.loss =  mse_loss\n",
    "\n",
    "    def optimization(self):\n",
    "        with tf.name_scope(\"optimization\"):\n",
    "            self.optim = self.optimizer.minimize(self.loss)\n",
    "\n",
    "    def eval(self):\n",
    "        \"\"\" Evaluate each sample.\"\"\"\n",
    "        self.se = tf.square(self.rating - self.prediction)\n",
    "\n",
    "\n",
    "    def build(self):\n",
    "        self.get_data()\n",
    "        self.inference()\n",
    "        self.create_model()\n",
    "        self.loss_func()\n",
    "        self.optimization()\n",
    "        self.eval()\n",
    "        self.saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "    def step(self, sess, step):\n",
    "        \"\"\" Train the model step by step. \"\"\"\n",
    "        if self.is_training:\n",
    "            loss, optim,se = sess.run([self.loss, self.optim,self.se])\n",
    "            return loss,np.sqrt(np.mean(se))\n",
    "        else:\n",
    "            se = sess.run([self.se])[0]\n",
    "            return se\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "embed_size = 5\n",
    "epochs = 20\n",
    "dataset = 'Baby'\n",
    "model_dir = './AttNCF9.5/'\n",
    "optim = 'Adam'\n",
    "activation = 'ReLU'\n",
    "lr = 0.002\n",
    "dropout = 0.5\n",
    "l2_rate = 0.001\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, user_size, item_size = read_data(dataset, embed_size, batch_size, True)\n",
    "test_data, user_size, item_size = read_data(dataset, embed_size, batch_size, False)\n",
    "iterator = tf.data.Iterator.from_structure(tf.compat.v1.data.get_output_types(train_data), tf.compat.v1.data.get_output_shapes(train_data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading model parameters from %s\n",
      "('For epoch: ', 0, ' Test RMSE is ', 1.1518992)\n",
      "('For epoch: ', 1, ' Test RMSE is ', 1.1608667)\n",
      "('For epoch: ', 2, ' Test RMSE is ', 1.1688697)\n",
      "('For epoch: ', 3, ' Test RMSE is ', 1.173174)\n",
      "('For epoch: ', 4, ' Test RMSE is ', 1.1713037)\n",
      "('For epoch: ', 5, ' Test RMSE is ', 1.1829746)\n",
      "('For epoch: ', 6, ' Test RMSE is ', 1.1860554)\n",
      "('For epoch: ', 7, ' Test RMSE is ', 1.1946323)\n",
      "('For epoch: ', 8, ' Test RMSE is ', 1.1950208)\n",
      "('For epoch: ', 9, ' Test RMSE is ', 1.1992342)\n",
      "('For epoch: ', 10, ' Test RMSE is ', 1.2151142)\n",
      "('For epoch: ', 11, ' Test RMSE is ', 1.2134184)\n",
      "('For epoch: ', 12, ' Test RMSE is ', 1.216163)\n",
      "('For epoch: ', 13, ' Test RMSE is ', 1.2192904)\n",
      "('For epoch: ', 14, ' Test RMSE is ', 1.2261074)\n",
      "('For epoch: ', 15, ' Test RMSE is ', 1.2359293)\n",
      "('For epoch: ', 16, ' Test RMSE is ', 1.2204726)\n",
      "('For epoch: ', 17, ' Test RMSE is ', 1.2364405)\n",
      "('For epoch: ', 18, ' Test RMSE is ', 1.2456203)\n",
      "('For epoch: ', 19, ' Test RMSE is ', 1.2436578)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(config=config) as sess:\n",
    "    model = AttNCF(user_size, item_size, embed_size,iterator, activation, lr, optim,l2_rate, dropout, is_training=True)\n",
    "    model.build()\n",
    "    ckpt = tf.train.get_checkpoint_state(model_dir)\n",
    "    if ckpt:\n",
    "        print(\"Reading model parameters from %s\".format(ckpt.model_checkpoint_path))\n",
    "        model.saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        print(\"Creating model with fresh parameters.\")\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    count = 0\n",
    "    for epoch in range(epochs):\n",
    "        sess.run(model.iterator.make_initializer(train_data))\n",
    "        model.is_training = True\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            batch_no= 0\n",
    "            while True:\n",
    "                loss,se = model.step(sess, count)\n",
    "#                 if(batch_no%100==0):\n",
    "#                     print(batch_no,loss,se)\n",
    "                batch_no+=1\n",
    "                count += 1\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass\n",
    "        \n",
    "        sess.run(model.iterator.make_initializer(test_data))\n",
    "        model.is_training = False\n",
    "        RMSE = np.array([], dtype=np.float32)\n",
    "        try:\n",
    "            while True:\n",
    "                se = model.step(sess, None)\n",
    "                RMSE = np.append(RMSE, se)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print(\"For epoch: \",epoch,\" Test RMSE is \",np.sqrt(RMSE.mean()))\n",
    "            \n",
    "        \n",
    "        checkpoint_path = os.path.join(model_dir, \"AttNCF.ckpt\")\n",
    "        model.saver.save(sess, checkpoint_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 90% of this code is from https://github.com/hustlingchen/A3NCF, https://github.com/guoyang9/A3NCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
